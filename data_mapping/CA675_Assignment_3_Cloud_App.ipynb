{"nbformat_minor": 0, "cells": [{"source": "# CA675 Assignment 3: 'Cloud Application'\n#Data Preparation\n_Disclaimer: Submitted to Dublin City University, School of Computing for module CA675: Cloud Technologies, 2016. We hereby certify that the work presented and the material contained herein is our own except where explicitly stated references to other material are made_.\n\n**Authors:** John Segrave, Paul O'Hara, Claire Breslin\n\n**Emails:** john.segravedaly2@mail.dcu.ie, paul.ohara6@mail.dcu.ie, claire.breslin4@mail.dcu.ie\n\n**Student IDs:** 14212108, 14212372, 14210826\n\n**Code & Data available online:**\n* **Code** -  Code and logs are in [this github repository](https://github.com/oharapaGitHub/ca675assignment3backend).\n * _You can download this data preparation notebook from GitHub and run it interactively in your own IPython Notebook._\n* **Data** - Raw, Processed and Final data can be found on [Google Drive](https://drive.google.com/a/mail.dcu.ie/folderview?id=0B4KWi5yG-4oGR1FQRXNOQ0VlYTA&usp=sharing).", "cell_type": "markdown", "metadata": {}}, {"source": "# Reader Insights with Wikipedia\n\n## 1. Introduction \n### The Idea\nYou're a news agency or a freelance writer. You already know what the big news story of the day is - e.g. The US Presidential race. There are so many angles you could potentially write about, but which one to pick?  Ideally, you would peek inside the mind of (a lot of) your audience and find out \"What angle **about this** do they find interesting **right now**? What do they want to know more about?\"\n\nClickstream data can give insights into this. When people want to know more about something, they look it up (e.g. on Wikipedia). Then they click on other pages telling them more about the angle they find most useful or interesting.\n\nWhat if we could make all those clicks visible? It would give content writers a window into the minds of readers - show the flow of their interests, their \u2018stream of consciousness\u2019  e.g. what can Wikipedia clickstream data tell us about the topics that people find interesting in the [Political_positions_of_Donald_Trump](www.wikipedia.org/wiki/Political_positions_of_Donald_Trump)? What other pages were they reading that led them there and what topics on that page did they find interesting enough to click on next?\n\n### The Data\n\nWikipedia recently started publishing clickstream data [here](https://figshare.com/articles/Wikipedia_Clickstream/1305770). The data shows how people get to a Wikipedia pages and what other pages they click on from that page.  Our application aims to use this data to give journalists an insight into \"what people are interested in right now\" in the context of a particular topic - a kind of window into the public _'(click) stream of consciousness'_ as it were.\n\nThe raw log data _\"contains counts of (referer, resource) pairs extracted from the request logs of Wikipedia\"_. At present, the only log available covers all of the (aggregated) clicks between pages in the 'en' wikipedia corpus for the month of March 2016- around 25 million log entries (1.2Gb).\n\nIdeally, we would like to have hourly (or finer) grained data, but while that is not available yet, the data to hand is perfectly suitable for a proof of concept. If the idea proves workable and useful, it could be extended to consume data at a finer level of granularity.  If time allows, we may experiment with streaming in new log entries as well.\n\n_Original data thanks to the [Wikipedia Research Community](https://meta.wikimedia.org/wiki/Research:Index) (Wulczyn, Ellery; Taraborelli, Dario (2016): Wikipedia Clickstream. Figshare. https://dx.doi.org/10.6084/m9.figshare.1305770.v16)_ ", "cell_type": "markdown", "metadata": {}}, {"source": "## 2. Data Preparation\n\nThe goal of the data preparation is to:\n1. Clean the raw log data - remove any unusable information.\n1. Turn the counts of (referer, resource) pairs into a fully-formed picture for each distinct wikipedia page as follows:\n * **Page Title:** The title of the wikipedia page (the string that comes after 'www.wikipedia.org/wiki/' in a wikipedia URL)\n * **Prioritised Inbound pages, with click counts**: A list of the pages (by title) that led to the current page, including a count of the number of clicks, ordered with the most interesting pages first (i.e. those with the highest click counts).\n * **Prioritised Outbound pages, with click counts** Similar to the previous list, but describing the pages on the current page - i.e. those that lead _out_ of the current page. Again, including a count of the number of clicks, ordered with the most interesting pages first (i.e. those with the highest click counts).\n\n**Technology**\nWe are using Apache Spark to process the raw log data for the simple reason that it has a good ecosystem of other potentially useful components which we may look into if there is time after the initial application is completed. For example: Spark Streaming and GraphX.\n\n## 3. Implementation\n### Initialise variables and load raw clickstream data into Spark", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "source = '2016_03_clickstream'                                    # full data Mar 2016\n#source = '2016_03_clickstream_sample_BRUSSELS_UTF8'               # medium test data - theme-centric (brussels)\n#source = '2016_03_clickstream_sample_TEST_DATASET'                # small test data for testing - handcrafted\n#source = '2016_03_clickstream_sample_BOWIE_UTF8'                  # medium test data - theme-centric (bowie)\n#source = '2016_03_clickstream_200000_random_rows_UTF8'            # medium, randomised test data\n\n# First files tried - turned out to be an experiment from 2015 with a different data format\n#source = '2015_02_clickstream'                                    # full data Feb 2015\n\ninfilename = source + '.tsv.gz'\noutfilename = source + '_RESULTS.tsv'\ninput_csv  = '/resources/' + infilename                            # Cloud location\noutput_csv = '/resources/' + outfilename                           # Cloud location\n#input_csv = 'c:/users/john@segrave.org/downloads/js_wikipedia_data/' + infilename     # local location\n#output_csv = 'c:/users/john@segrave.org/downloads/js_wikipedia_data/' + outfilename   # local location\n\n# Point Spark at the full, compressed log data - this actual file will not be read until\n# one of our Spark actions requires it (lazy loading).\n# Metadata: https://datahub.io/dataset/wikipedia-clickstream/resource/be85cc68-d1e6-4134-804a-fd36b94dbb82\ntsv_lines_rdd = sc.textFile(input_csv)\n\n# And we'll cause the file to be read straight away by counting the number of log entries :-)\nprint(\"Total number of log entries: %d\" % (tsv_lines_rdd.count()))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Total number of log entries: 25617311\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Create the basic 'raw data' RDD from which other processing will follow", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "# WIKIPEDIA CHANGED THE FILE FORMAT: THIS IS THE OLD FORMAT. NEW ONE FOR MARCH 2016 FOLLOWS... \n# Chop the data into tokens, perform datatype conversions and \n# It would probably be more efficient not to have the CSV header in the file to begin with, but we don't get to control that.\n#raw_data_rdd = (tsv_lines_rdd\n#         .filter(lambda line : not(line.startswith('prev_id')))               # Filter out the CSV header\n#         .map(lambda line : line.split('\\t'))                                 # Tokenise the input data\n#         .map(lambda (prev_id, curr_id, n, prev_title, curr_title)\n#              : (prev_id, curr_id, int(n), prev_title, curr_title)) # Datatype conversion\n#         ).cache()                                                            # Cache the result because we'll use it a fair bit", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 3, "cell_type": "code", "source": "# 'other-' pages just fill up the inbound list without adding value, so we'll\n# collapse them into a single record to preserve accurate link counts & proportions.\nother_title = \"(external)_websearch_social_media_etc\"\n\n# First level of data cleaning, eliminate 'noise' pages that clog up the UI.\ndef eliminate_noise_pages(line) :\n    # Tokenise the input data\n    prev_title, curr_title, ref_type, n = line.split('\\t')\n    # Filter 'noise' pages by giving them all a mocked-up common page name (this preserves accurate click counts).\n    prev_title_cleaned = (other_title if (ref_type == 'external') else prev_title)\n    # Return filtered pages and do type conversion\n    return (prev_title_cleaned, curr_title, int(n))\n\n# Chop the data into tokens, perform datatype conversions and \n# It would probably be more efficient not to have the CSV header in the file to begin with, but we don't get to control that.\n# WIKIPEDIA CHANGED THE FILE FORMAT: THIS IS THE NEW FORMAT FOR MARCH 2016 \nraw_data_rdd = (tsv_lines_rdd\n         # Filter out the CSV header\n         .filter(lambda line : not(line.startswith('prev\\t')))\n\n         # Filter 'noise' pages and do type conversion\n         .map(eliminate_noise_pages)\n\n         # Map/reduce multiple noise page references down to a single page with aggregate count\n         .map(lambda (prev_title, curr_title, n) : ((prev_title, curr_title), n))\n         .reduceByKey(lambda n1, n2 : n1 + n2)\n         .map(lambda ((prev_title, curr_title), n) : (prev_title, curr_title, n))\n                \n         # Cache the result because we'll use it a fair bit\n         ).cache()\n\nraw_data_rdd.takeSample(False, 1) # for dev/test, remove on production run", "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": "[(u'Minister-President_of_the_Brussels-Capital_Region', u'Rudi_Vervoort', 127)]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Collect the list of pages that referred *into* every individual page (with corresponding click counts)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "# WIKIPEDIA CHANGED THE FILE FORMAT: THIS IS THE OLD FORMAT, NEED NEW ONE FOR MARCH 2016 \n#inbound_links_rdd = (raw_data_rdd\n#\n#         # Drop the ID fields as their quality is poor\n#         .map(lambda (prev_id, curr_id, n, prev_title, curr_title, ref_type)\n#              : (curr_title, ([prev_title], [n])))\n#\n#         # Create two lists - pages and coresponding clicks\n#         .reduceByKey( lambda x, y: (x[0]+y[0],x[1]+y[1]))\n#    )", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 6, "cell_type": "code", "source": "# WIKIPEDIA CHANGED THE FILE FORMAT: THIS IS THE NEW FORMAT FOR MARCH 2016 \ninbound_links_rdd = (raw_data_rdd\n         .map(lambda (prev_title, curr_title, n)\n              : (curr_title, ([prev_title], [n])))\n\n         # Create two lists - pages and coresponding clicks\n         .reduceByKey( lambda x, y: (x[0]+y[0],x[1]+y[1]))\n    )\ninbound_links_rdd.takeSample(False, 1) # for dev/test, remove on production run", "outputs": [{"execution_count": 7, "output_type": "execute_result", "data": {"text/plain": "[(u'European_School,_Brussels_III',\n  ([u'European_School',\n    u'List_of_international_schools',\n    '(external)_websearch_social_media_etc'],\n   [25, 11, 153]))]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Collect the list of pages that are referred *out* from every individual page (with corresponding click counts)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "# WIKIPEDIA CHANGED THE FILE FORMAT: THIS IS THE OLD FORMAT, NEED NEW ONE FOR MARCH 2016 \n#outbound_links_rdd = (raw_data_rdd\n#\n#         # Drop the ID fields as their quality is poor\n#         .map(lambda (prev_id, curr_id, n, prev_title, curr_title)\n#              : (prev_title, ([curr_title], [n])))\n#\n#         # Create two lists - pages and coresponding clicks\n#         .reduceByKey( lambda x, y: (x[0]+y[0],x[1]+y[1]))\n#    )", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 9, "cell_type": "code", "source": "outbound_links_rdd = (raw_data_rdd\n         .map(lambda (prev_title, curr_title, n)\n              : (prev_title, ([curr_title], [n])))\n\n         # Create two lists - pages and coresponding clicks\n         .reduceByKey( lambda x, y: (x[0]+y[0],x[1]+y[1]))\n    )\noutbound_links_rdd.takeSample(False, 1) # for dev/test, remove on production run", "outputs": [{"execution_count": 10, "output_type": "execute_result", "data": {"text/plain": "[(u'Copenhagen_Airport',\n  ([u'Brussels_Airport',\n    u'Brussels_South_Charleroi_Airport',\n    u'Brussels_Airlines'],\n   [44, 13, 13]))]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### This function that prioritises the inbound and outbound page lists\nSort them in descending order of click counts", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "from operator import itemgetter\n\nmax_length = 100 # truncate lists after this length, as there can be a very long tail of uninteresting low-click pages\n\n# Had to do some messing in Spark to emit the correct list shape after joining & sorting.We'll use this\n# function,in a map shortly.\ndef sort_2_lists(page_title, in_titles, in_counts, out_titles, out_counts) : \n    if len(in_counts) == 0 : # can happen, depending on the join characteristics\n        in_counts2 = []\n        in_titles2 = []\n    else:\n        # Pair the page title and count lists, sorts by the counts and emit a new pair of (sorted) lists\n        in_titles2, in_counts2 = (list(a) for a in zip(*sorted(zip(in_titles, in_counts), key=itemgetter(1), reverse=True)))\n    if len(out_counts) == 0 :\n        out_counts2 = [] # can happen, depending on the join characteristics\n        out_titles2 = []\n    else:\n        # Pair the page title and count lists, sorts by the counts and emit a new pair of sorted & truncated lists\n        out_titles2, out_counts2 = (list(a) for a in zip(*sorted(zip(out_titles, out_counts), key=itemgetter(1), reverse=True)))\n    return page_title, in_titles2[0:max_length], in_counts2[0:max_length], out_titles2[0:max_length], out_counts2[0:max_length]\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Join the inbound and outbound datasets for every page to form a single overall picture of the flow for each page.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "def check_tuple(tup): return ([],[]) if tup is None else tup\n\n# Join the inbound and outbound datasets for every page to form a single overall picture of the flow for each page.\nall_links_rdd = (inbound_links_rdd\n\n    # Pull it all together into a single dataset, using the page Title as the key on which to join the two datasets. \n    .rightOuterJoin(outbound_links_rdd)\n\n    # Have to do some messing to emit the correct list shape after joining.\n    .map(lambda (page_title, (in_tup, out_tup))\n         : ((page_title), check_tuple(in_tup), check_tuple(out_tup)))\n\n    # Sort the list by referral count.\n    # Have to do some more messing to emit the correct list shape after sorting.\n    .map(lambda ((page_title), (in_titles, in_counts), (out_titles, out_counts))\n         : sort_2_lists(page_title, in_titles, in_counts, out_titles, out_counts))\n    )\n\nall_links_rdd.cache()\n", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "PythonRDD[171] at RDD at PythonRDD.scala:43"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Have a look at the result - a complete picture of the page flow for one page", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "all_links_rdd.takeSample(False, 1) # for dev/test, remove on production run", "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "[(u'Heysel/Heizel_metro_station',\n  [u'Brussels_Metro',\n   u'Brussels_Metro_line_6',\n   u'List_of_Brussels_Metro_stations'],\n  [72, 15, 13],\n  [u'Brussels_Metro_line_6'],\n  [18])]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# 4. Save the output\n\nNote: if we had a complete development pipeline here, we would be writing this back to our application database, not to CSV. However, we do not have a single integrated environment that has Hadoop, Spark, iPython, an application database and a web application server all neatly integrated!  So for this proof of concept application, we will get the data from Hadoop/Spark to our application database via an exported file. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "def to_tsv(page_title, in_titles, in_counts, out_titles, out_counts):\n    # Write the inbound & outbound titles and count lists in a form that the web app can easily consume\n    in_titles_str = '[' + ' '.join(in_titles) + ']'\n    in_counts_str = '[' + ' '.join([str(count) for count in in_counts]) + ']'\n    out_titles_str = '[' + ' '.join(out_titles) + ']'\n    out_counts_str = '[' + ' '.join([str(count) for count in out_counts]) + ']'\n    # Return it as a tab separated line for a .tsv file (not comma-separated, as page titles can contain commas)\n    return '\\t'.join([page_title, in_titles_str, in_counts_str, out_titles_str, out_counts_str])\n\n# Export the finalised data to a .tsv (in real life, with a full pipeline, we'd inject this into the DB directly)\ncsv_rdd = all_links_rdd.map(lambda x : to_tsv(x[0], x[1], x[2], x[3], x[4]))\ncsv_rdd.saveAsTextFile(output_csv)\n\n# Check the output\n! ls -la /resources/*{source}*", "outputs": [{"output_type": "stream", "name": "stdout", "text": "-rw-rw-r-- 1 notebook resources 254006007 Apr  8 07:47 /resources/2016_03_clickstream_RESULTS.tsv.gz\r\n-rw-rw-r-- 1 notebook resources       379 Apr 11 20:37 /resources/2016_03_clickstream_sample_TEST_DATASET_RESULTS.tsv.gz\r\n-rw-rw-r-- 1     2003 resources       348 Apr  8 12:56 /resources/2016_03_clickstream_sample_TEST_DATASET.tsv.gz\r\n-rw-rw-r-- 1     2003 resources 393452921 Apr  4 20:44 /resources/2016_03_clickstream.tsv.gz\r\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# 5. Testing\n\n## Testing Approach\nAs noted in Assignment 2, testing 'Big Data' solutions in the wild can be difficult. How can we know if the results we obtained are the correct results?\n\nIn this example, there are no other solutions to exchange results and compare with. So we create a set of \"expected\" results by *manually* replicating our data preparation on a small subset of the data. We then compare that to the 'actual' results produced by our data preparation code above and (for the same small dataset) and ensure that the two match.\n\nThis helps enormously with initial debugging, but the main value is in long-term automated testing. Armed with the 'expected' results dataset, we can then confidently make changes to our main analysis code (e.g. to refactor, improve performance, etc). We just re-run the new code, using the same sample and confirm that our actual results still match the 'expected' ones.\n\n## Testing the wikipedia clickstream data\nTo test our data preparation code, we take 15 sample wikipedia clickstream log entries and treat them as a mini clickstream dataset. We sample pages around the theme of the 2016 Brussels bombings and select two main pages to focus on: [2016_Brussels_bombings](http://en.wikipedia.org/wiki/2016_Brussels_bombings) and [Reactions_to_the_2016_Brussels_bombings](http://en.wikipedia.org/wiki/Reactions_to_the_2016_Brussels_bombings). Within these pages, we select a comprehensive mix of all log entry types:\n* Four regular click log entries - where one of our selected pages is either the 'current' page or the page that referred to it.\n* Four log entries that are referred from an 'other' source - wikipedia pages that do *not* link directly to the page in question. This usually indicates the user ran a search from the referring page.\n* Three log entries for each page that are referred from an 'external' source - such as Google, Facebook, etc.\n* In the above entries, a good mix of our pages playing both 'referrer' and 'referred to' role.\n\nTest dataset: See the [TEST_DATA](https://drive.google.com/drive/u/1/folders/0B4KWi5yG-4oGR1FQRXNOQ0VlYTA) folder on Google Drive.\n\nWe (manually) work out the expected content of each row in our final dataset:\n* The pages - these  will form the Primary Key of our finalised data.\n* The set of pages that were clicked through to a given page (in descending order of clicks).\n* The set of pages that were clicked on to exit a given page (again, in descending order of clicks).\n* We also perform some data cleaning along the way - e.g. consolidating the 'external' referer pages as they are not terribly interesting in the final application.\n\nWe then compare those expected results to an actual code run (using the same data) and check that these 'actual' results match the 'expected' ones. \n\n# Test Results\n\nThe full manual test scenario walk-through is quite involved, so it is provided separately to this Notebook (in the [test_data_clickstream_EXPECTED_RESULT.txt](https://github.com/oharapaGitHub/ca675assignment3backend/blob/master/data_mapping/test_data_clickstream_EXPECTED_RESULT.txt) file on Github.\n\nHere is an excerpt from the test scenario walk-through. The EXPECTED row was calculated manually from the test dataset and the ACTUAL row was obtained by running the above Spark application on the same dataset. The results are identical, so we have some confidence that our code is producing the correct results.\n\n```\nEXPECTED :\n2016_Brussels_bombings\t[(external)_websearch_social_media_etc Main_Page Reactions_to_the_2016_Brussels_bombings Timeline_of_ISIL-related_events_(2016) Belgium_national_football_team 2016_Lahore_suicide_bombing Half-mast Belgium Schuman_metro_station]\t[292235 145244 1523 985 155 145 69 53 48]\t[Reactions_to_the_2016_Brussels_bombings]\t[29799]\n\nACTUAL   :\n2016_Brussels_bombings\t[(external)_websearch_social_media_etc Main_Page Reactions_to_the_2016_Brussels_bombings Timeline_of_ISIL-related_events_(2016) Belgium_national_football_team 2016_Lahore_suicide_bombing Half-mast Belgium Schuman_metro_station]\t[292235 145244 1523 985 155 145 69 53 48]\t[Reactions_to_the_2016_Brussels_bombings]\t[29799]\nRESULT   : CONFIRMED\n```", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}